{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115551a-14b4-4eec-9a4b-19dfb42a34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSense Scenario 36 64 Beams: Model Fusion GPS, EffNetB0-B7, PointCloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e25db-e114-4484-bb8b-28e4ff7423af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch as t\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transf\n",
    "from torchvision import transforms, utils\n",
    "from torchsummary import summary\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage import io\n",
    "import ast\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "from plyfile import PlyData, PlyElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5a236-3de5-4f02-97ab-0d2319a70a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Model\n",
    "# EfficientNet B0-B7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02977c-e0fe-490c-9678-4bb16bb41b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]\n",
    "\n",
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU()  # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.silu(self.bn(self.cnn(x)))\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # C x H x W -> C x 1 x 1\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        expand_ratio,\n",
    "        reduction=4,  # squeeze excitation\n",
    "        survival_prob=0.8,  # for stochastic depth\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.survival_prob = 0.8\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                in_channels,\n",
    "                hidden_dim,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                hidden_dim,\n",
    "                hidden_dim,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=hidden_dim,\n",
    "            ),\n",
    "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def stochastic_depth(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = (\n",
    "            torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        )\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "\n",
    "        if self.use_residual:\n",
    "            return self.stochastic_depth(self.conv(x)) + inputs\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, version):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
    "        last_channels = ceil(1280 * width_factor)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, 128),\n",
    "        )\n",
    "\n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha**phi\n",
    "        width_factor = beta**phi\n",
    "        return width_factor, depth_factor, drop_rate\n",
    "\n",
    "    def create_features(self, width_factor, depth_factor, last_channels):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4 * ceil(int(channels * width_factor) / 4)\n",
    "            layers_repeats = ceil(repeats * depth_factor)\n",
    "\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride=stride if layer == 0 else 1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,  # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "        features.append(\n",
    "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.features(x))\n",
    "        return self.classifier(x.view(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed653669-7ea5-4e1b-a24e-163817358a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed CNN Model for GPS\n",
    "\n",
    "class NN_beam_pred(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(NN_beam_pred, self).__init__()\n",
    "\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # Defining the convolution layer\n",
    "            nn.Conv1d(num_features, 30, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(30, 20, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(20, 10, kernel_size=2, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(10, 30, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(30, 20, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(20, 10, kernel_size=2, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(10, 30, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(30, 20, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(20, 10, kernel_size=2, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.l1 = nn.Linear(20, 128)\n",
    "        self.l2 = nn.Linear(128, 128)\n",
    "        # self.l3 = nn.Linear(128, num_output)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.cnn_layers(inputs)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        # x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8cc57-228b-46c7-bf97-1d361ed876ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PointNet Model for LiDAR Data\n",
    "\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super(TNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 15000*k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # identity_matrix = torch.eye(15000, device=x.device).view(1, 3, 15000).repeat(x.size(0), 1, 1)\n",
    "        x = x.view(-1, 15000, 3) #+ identity_matrix\n",
    "        return x\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.input_transform = TNet(k=3)  # Input shape is (3, 3)\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply input transformation\n",
    "        transformation = self.input_transform(x)\n",
    "        x = torch.bmm(x, transformation)  # Apply the transformation to input points\n",
    "\n",
    "        # PointNet layers\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82ec93-ebf8-416b-a8dc-b7b2fbe16060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Testing, and Validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc51bc0-0f2b-43bb-aba1-552312e794af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper-parameters\n",
    "batch_size = 16\n",
    "val_batch_size = 1\n",
    "lr = 0.01\n",
    "decay = 1e-4\n",
    "num_epochs = 40\n",
    "train_size = [1]\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = 2\n",
    "node = 512\n",
    "output_size = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093e6ee-d4d7-4864-9873-03cedff7927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(root, shuffle=False, nat_sort=False):\n",
    "    f = pd.read_csv(root)\n",
    "    data_samples = []\n",
    "    pred_val = []\n",
    "    for idx, row in f.iterrows():\n",
    "        data = list(row.values[1:])\n",
    "        data_samples.append(data)\n",
    "\n",
    "    return data_samples\n",
    "\n",
    "\n",
    "class DataFeed(Dataset):\n",
    "    '''\n",
    "    A class retrieving a tuple of (image,label). It can handle the case\n",
    "    of empty classes (empty folders).\n",
    "    '''\n",
    "    def __init__(self, image_dir, pos_dir, lidar_dir, nat_sort = False, num_points=15000, image_transform=None, pos_transform = None, init_shuflle = True):\n",
    "        self.root1 = image_dir\n",
    "        self.root2 = pos_dir\n",
    "        self.root3 = lidar_dir\n",
    "\n",
    "        self.num_points = num_points\n",
    "\n",
    "        self.samples1 = create_samples(self.root1, shuffle=init_shuflle, nat_sort=nat_sort)\n",
    "        self.samples2 = create_samples(self.root2, shuffle=init_shuflle, nat_sort=nat_sort)\n",
    "        self.samples3 = create_samples(self.root3, shuffle=init_shuflle, nat_sort=nat_sort)\n",
    "        self.transform1 = image_transform\n",
    "        self.transform2 = pos_transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample1 = self.samples1[idx]\n",
    "        img = io.imread(sample1[0])\n",
    "        if self.transform1:\n",
    "            img = self.transform1(img)\n",
    "        label = sample1[1]\n",
    "\n",
    "        ################################\n",
    "        sample2 = self.samples2[idx]\n",
    "\n",
    "        pos_val = sample2[:1]\n",
    "        pos_val = ast.literal_eval(pos_val[0])\n",
    "        pos_val = np.asarray(pos_val)\n",
    "        ################################\n",
    "        sample3 = self.samples3[idx]\n",
    "        df = pd.read_csv(sample3[0])\n",
    "        # Extract the columns of interest\n",
    "        x = df[' X (mm)'] / 1000  # Convert to meters\n",
    "        y = df[' Y (mm)'] / 1000\n",
    "        z = df[' Z (mm)'] / 1000\n",
    "        points = np.column_stack((x.values, y.values, z.values))\n",
    "        if points.shape[0] < self.num_points:\n",
    "            points = np.pad(points, ((0, self.num_points - points.shape[0]), (0, 0)), mode='constant')\n",
    "        elif points.shape[0] > self.num_points:\n",
    "            indices = np.random.choice(points.shape[0], self.num_points, replace=False)\n",
    "            points = points[indices]\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "        # pos_centers = sample2[1:2]\n",
    "        # pos_centers = np.asarray(pos_centers)\n",
    "\n",
    "        return ([img, pos_val, points.T], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c18bb3-4efe-4ee3-9a55-ba17ea4c1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transform\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "image_proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "\n",
    "\n",
    "#POS transform\n",
    "pos_proc_pipe = transf.Compose(\n",
    "    [\n",
    "        transf.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d8541-065d-491b-9968-09478b3859bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year month day\n",
    "dayTime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "# Minutes and seconds\n",
    "hourTime = datetime.datetime.now().strftime('%H_%M')\n",
    "print(dayTime + '\\n' + hourTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a421d8-75fc-4683-89f8-050f8193dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd() + '//' + 'saved_folder' + '//' + dayTime + '_' + hourTime\n",
    "print(pwd)\n",
    "# Determine whether the folder already exists\n",
    "isExists = os.path.exists(pwd)\n",
    "if not isExists:\n",
    "    os.makedirs(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7513bdf-c128-406b-a831-df6acc8599a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training files to the saved directory\n",
    "shutil.copy('./scenario36_64_img_beam_train.csv', pwd)\n",
    "shutil.copy('./scenario36_64_img_beam_val.csv', pwd)\n",
    "shutil.copy('./scenario36_64_img_beam_test.csv', pwd)\n",
    "shutil.copy('./scenario36_64_lidar_beam_train.csv', pwd)\n",
    "shutil.copy('./scenario36_64_lidar_beam_val.csv', pwd)\n",
    "shutil.copy('./scenario36_64_lidar_beam_test.csv', pwd)\n",
    "shutil.copy('./scenario36_64_pos_beam_train.csv', pwd)\n",
    "shutil.copy('./scenario36_64_pos_beam_val.csv', pwd)\n",
    "shutil.copy('./scenario36_64_pos_beam_test.csv', pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d29978-423d-4cca-b909-c6f3ba26e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_dir = './scenario36_64_img_beam_train.csv'\n",
    "image_val_dir = './scenario36_64_img_beam_val.csv'\n",
    "\n",
    "pos_train_dir = './scenario36_64_pos_beam_train.csv'\n",
    "pos_val_dir = './scenario36_64_pos_beam_val.csv'\n",
    "\n",
    "lidar_train_dir = './scenario36_64_lidar_beam_train.csv'\n",
    "lidar_val_dir = './scenario36_64_lidar_beam_val.csv'\n",
    "\n",
    "train_loader = DataLoader(DataFeed(image_train_dir, pos_train_dir, lidar_train_dir, image_transform=image_proc_pipe),\n",
    "                          batch_size=batch_size,\n",
    "                          #num_workers=8,\n",
    "                          shuffle=False)\n",
    "val_loader = DataLoader(DataFeed(image_val_dir, pos_val_dir, lidar_train_dir, image_transform=image_proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00535f2e-bbf0-48e4-b987-0a328202cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = pwd + '//' + 'saved_analysis_files'\n",
    "checkpoint_directory = pwd + '//' + 'checkpoint'\n",
    "\n",
    "isExists = os.path.exists(save_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "isExists = os.path.exists(checkpoint_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bd1ca-cf8b-428f-8c2b-f7bce3cb6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fusion_model(nn.Module):\n",
    "    def __init__(self, version, num_features, num_classes):\n",
    "        super(fusion_model, self).__init__()\n",
    "        self.nclass = num_classes\n",
    "\n",
    "        self.nf = num_features\n",
    "        self.version = version\n",
    "\n",
    "        self.eff = EfficientNet(self.version)\n",
    "        self.cnv = NN_beam_pred(self.nf)\n",
    "        self.pnet = PointNet(num_classes=self.nclass)\n",
    "\n",
    "        self.fc1 = nn.Linear(321, 256)\n",
    "        self.fc2 = nn.Linear(256, self.nclass)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.eff(inputs[0])\n",
    "        y = self.cnv(inputs[1].reshape([-1, 2, 1]))\n",
    "        z = self.pnet(inputs[2])\n",
    "\n",
    "        x = torch.cat([x, y, z], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = fusion_model(\"b0\", input_size, num_classes=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffedffa-1913-44fb-8ac8-e67846489535",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = []\n",
    "\n",
    "with cuda.device(0):\n",
    "    top_1 = np.zeros( (1,len(train_size)) )\n",
    "    top_3 = np.zeros( (1,len(train_size)) )\n",
    "    top_5 = np.zeros( (1,len(train_size)) )\n",
    "    top_7 = np.zeros( (1,len(train_size)) )\n",
    "    top_9 = np.zeros( (1,len(train_size)) )\n",
    "    top_11 = np.zeros( (1,len(train_size)) )\n",
    "    top_13 = np.zeros( (1,len(train_size)) )\n",
    "    top_15 = np.zeros( (1,len(train_size)) )\n",
    "    acc_loss = 0\n",
    "    itr = []\n",
    "    for idx, n in enumerate(train_size):\n",
    "        print('```````````````````````````````````````````````````````')\n",
    "        print('Training size is {}'.format(n))\n",
    "        # Build the network:\n",
    "        net = model.cuda()\n",
    "        layers = list(net.children())\n",
    "\n",
    "        #  Optimization parameters:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        opt = optimizer.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "        LR_sch = optimizer.lr_scheduler.MultiStepLR(opt, [15,25,40], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "        count = 0\n",
    "        running_loss = []\n",
    "        running_top1_acc = []\n",
    "        running_top3_acc = []\n",
    "        running_top5_acc = []\n",
    "        running_top7_acc = []\n",
    "        running_top9_acc = []\n",
    "        running_top11_acc = []\n",
    "        running_top13_acc = []\n",
    "        running_top15_acc = []\n",
    "\n",
    "        best_accuracy = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch No. ' + str(epoch + 1))\n",
    "            skipped_batches = 0\n",
    "            for tr_count, (pos_data, beam_val) in enumerate(train_loader):\n",
    "                net.train()\n",
    "                data = [pos_data[0], pos_data[1].type(torch.Tensor), pos_data[2].type(torch.Tensor)]\n",
    "                label = beam_val.type(torch.LongTensor)\n",
    "                x = [data[0].cuda(), data[1].cuda(), data[2].cuda()]\n",
    "                #print(\"x size\", x.size())\n",
    "                opt.zero_grad()\n",
    "                label = label.cuda()\n",
    "                #print(\"label size\", label.size())\n",
    "                # print(x.shape)\n",
    "                out = net.forward(x)\n",
    "                # print(out.shape, label.shape)\n",
    "                loss = criterion(out, label)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                batch_loss = loss.item()\n",
    "                acc_loss += batch_loss\n",
    "                count += 1\n",
    "                if np.mod(count, 100) == 0:\n",
    "                    print('Training-Batch No.' + str(count))\n",
    "                    running_loss.append(batch_loss)  # running_loss.append()\n",
    "                    itr.append(count)\n",
    "                    print('Loss = ' + str(running_loss[-1]))\n",
    "\n",
    "            print('Start Validation')\n",
    "            ave_top1_acc = 0\n",
    "            ave_top3_acc = 0\n",
    "            ave_top5_acc = 0\n",
    "            ave_top7_acc = 0\n",
    "            ave_top9_acc = 0\n",
    "            ave_top11_acc = 0\n",
    "            ave_top13_acc = 0\n",
    "            ave_top15_acc = 0\n",
    "            ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "            top1_pred_out = []\n",
    "            top3_pred_out = []\n",
    "            top5_pred_out = []\n",
    "            top7_pred_out = []\n",
    "            top9_pred_out = []\n",
    "            top11_pred_out = []\n",
    "            top13_pred_out = []\n",
    "            top15_pred_out = []\n",
    "            total_count = 0\n",
    "\n",
    "            gt_beam = []\n",
    "            for val_count, (pos_data, beam_val) in enumerate(val_loader):\n",
    "                net.eval()\n",
    "                data = [pos_data[0], pos_data[1].type(torch.Tensor), pos_data[2].type(torch.Tensor)]\n",
    "                label = beam_val.type(torch.LongTensor)\n",
    "                x = [data[0].cuda(), data[1].cuda(), data[2].cuda()]\n",
    "                #print(\"x size\", x.size())\n",
    "                opt.zero_grad()\n",
    "                labels = label.cuda()\n",
    "                gt_beam.append(labels.detach().cpu().numpy()[0].tolist())\n",
    "                total_count += labels.size(0)\n",
    "                out = net.forward(x)\n",
    "                _, top_1_pred = t.max(out, dim=1)\n",
    "                top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "                sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "                top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "                top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "                top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "                top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "                top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "                top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "                top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "                top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "                tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "                tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "                tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "                tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "                tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "                tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "                tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "                batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "                batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "                batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "                batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "                batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "                batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "                batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "                batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "                ave_top1_acc += batch_top1_acc.item()\n",
    "                ave_top3_acc += batch_top3_acc.item()\n",
    "                ave_top5_acc += batch_top5_acc.item()\n",
    "                ave_top7_acc += batch_top7_acc.item()\n",
    "                ave_top9_acc += batch_top9_acc.item()\n",
    "                ave_top11_acc += batch_top11_acc.item()\n",
    "                ave_top13_acc += batch_top13_acc.item()\n",
    "                ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "            print(\"total test examples are\", total_count)\n",
    "            running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "            running_top3_acc.append(ave_top3_acc / total_count)\n",
    "            running_top5_acc.append(ave_top5_acc / total_count)\n",
    "            running_top7_acc.append(ave_top7_acc / total_count)\n",
    "            running_top9_acc.append(ave_top9_acc / total_count)\n",
    "            running_top11_acc.append(ave_top11_acc / total_count)\n",
    "            running_top13_acc.append(ave_top13_acc / total_count)\n",
    "            running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "            print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "            print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "            print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "            print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "            print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "            print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "            print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "            print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "            print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "            cur_accuracy  = running_top1_acc[-1]\n",
    "\n",
    "            print(\"current acc\", cur_accuracy)\n",
    "            print(\"best acc\", best_accuracy)\n",
    "            if cur_accuracy > best_accuracy:\n",
    "                print(\"Saving the best model\")\n",
    "                net_name = checkpoint_directory  + '//' +  'fused_nn_beam_pred'\n",
    "                t.save(net.state_dict(), net_name)\n",
    "                best_accuracy =  cur_accuracy\n",
    "            print(\"updated best acc\", best_accuracy)\n",
    "\n",
    "\n",
    "            print(\"Saving the predicted value in a csv file\")\n",
    "            file_to_save = f'{save_directory}//topk_fused_val_after_{epoch+1}th_epoch.csv'\n",
    "            indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "            df1 = pd.DataFrame()\n",
    "            df1['index'] = indx\n",
    "            df1['link_status'] = gt_beam\n",
    "            df1['top1_pred'] = top1_pred_out\n",
    "            df1['top3_pred'] = top3_pred_out\n",
    "            df1['top5_pred'] = top5_pred_out\n",
    "            df1['top7_pred'] = top7_pred_out\n",
    "            df1['top9_pred'] = top9_pred_out\n",
    "            df1['top11_pred'] = top11_pred_out\n",
    "            df1['top13_pred'] = top13_pred_out\n",
    "            df1['top15_pred'] = top15_pred_out\n",
    "            df1.to_csv(file_to_save, index=False)\n",
    "\n",
    "            LR_sch.step()\n",
    "\n",
    "        top_1[0,idx] = running_top1_acc[-1]\n",
    "        top_3[0,idx] = running_top3_acc[-1]\n",
    "        top_5[0,idx] = running_top5_acc[-1]\n",
    "        top_7[0,idx] = running_top7_acc[-1]\n",
    "        top_9[0,idx] = running_top9_acc[-1]\n",
    "        top_11[0,idx] = running_top11_acc[-1]\n",
    "        top_13[0,idx] = running_top13_acc[-1]\n",
    "        top_15[0,idx] = running_top15_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83898e-cb09-4942-857d-efeeac842201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start Validation')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "running_top1_acc = []\n",
    "running_top3_acc = []\n",
    "running_top5_acc = []\n",
    "running_top7_acc = []\n",
    "running_top9_acc = []\n",
    "running_top11_acc = []\n",
    "running_top13_acc = []\n",
    "running_top15_acc = []\n",
    "total_count = 0\n",
    "\n",
    "gt_beam = []\n",
    "\n",
    "for val_count, (pos_data, beam_val) in enumerate(val_loader):\n",
    "    net.eval()\n",
    "    data = [pos_data[0], pos_data[1].type(torch.Tensor), pos_data[2].type(torch.Tensor)]\n",
    "    label = beam_val.type(torch.LongTensor)\n",
    "    x = [data[0].cuda(), data[1].cuda(), data[2].cuda()]\n",
    "    #print(\"x size\", x.size())\n",
    "    opt.zero_grad()\n",
    "    labels = label.cuda()\n",
    "\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0].tolist())\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x)\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "    batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "    batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "    batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "    batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "    batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "print(\"total examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval.csv'\n",
    "\n",
    "indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = indx\n",
    "df2['link_status'] = gt_beam  # Add the link_status column\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1e502-07e1-47c7-baea-d3c65211b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, need to work from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10251fd8-a4ee-41a3-8780-ff667510db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "image_test_dir = './scenario36_64_img_beam_test.csv'\n",
    "pos_test_dir = './scenario36_64_pos_beam_test.csv'\n",
    "lidar_test_dir = './scenario36_64_lidar_beam_test.csv'\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(image_test_dir)\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "link_status_data = test_data['original_unit1_pwr3_best-beam'].tolist()\n",
    "org = test_data['original_index'].tolist()\n",
    "pwr_60ghz = test_data['original_unit1_pwr3'].tolist()\n",
    "\n",
    "checkpoint_path = f'{checkpoint_directory}/fused_nn_beam_pred'\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "net = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ced97a-c159-4d15-990b-f5cb7609da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(DataFeed(image_test_dir, pos_test_dir, lidar_test_dir, image_transform=image_proc_pipe),\n",
    "                            batch_size=val_batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c746aa3-4547-46e6-9f0d-dc486102617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start Testing')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "running_top1_acc = []\n",
    "running_top3_acc = []\n",
    "running_top5_acc = []\n",
    "running_top7_acc = []\n",
    "running_top9_acc = []\n",
    "running_top11_acc = []\n",
    "running_top13_acc = []\n",
    "running_top15_acc = []\n",
    "total_count = 0\n",
    "\n",
    "gt_beam = []\n",
    "\n",
    "for val_count, (pos_data, beam_val) in enumerate(test_loader):\n",
    "    net.eval()\n",
    "    data = [pos_data[0], pos_data[1].type(torch.Tensor), pos_data[2].type(torch.Tensor)]\n",
    "    label = beam_val.type(torch.LongTensor)\n",
    "    x = [data[0].cuda(), data[1].cuda(), data[2].cuda()]\n",
    "    #print(\"x size\", x.size())\n",
    "    opt.zero_grad()\n",
    "    labels = label.cuda()\n",
    "\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0].tolist())\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x)\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "    batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "    batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "    batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "    batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "    batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "print(\"total examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_Test.csv'\n",
    "\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "# link_status_data = test_data['original_unit1_pwr3_best-beam'].tolist()\n",
    "# org = test_data['original_index'].tolist()\n",
    "# pwr_60ghz = test_data['original_unit1_pwr3'].tolist()\n",
    "\n",
    "indx = test_data.index + 1\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = org\n",
    "df2['link_status'] = link_status_data  # Add the link_status column\n",
    "df2['original_unit1_pwr3'] = pwr_60ghz # Add the original_unit1_pwr_60ghz column\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
